{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "nAnchors = 24\n",
    "batchSize = 16\n",
    "numEpochs = 3\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(model, df, fileDir, size=(224,224), outfile=None, nImages=330):\n",
    "    newDf = df.copy()\n",
    "    newDf.loc[:,\"Features\"] = np.zeros((newDf.shape[0], 2208)).tolist()\n",
    "    newDf.loc[:,\"Features\"] = newDf.loc[:,\"Features\"].astype(object)\n",
    "    \n",
    "    # define new index for images where first significant digit is\n",
    "    # sequence number, thus it has to have more overall digits \n",
    "    # than max image number. \n",
    "    newDf.loc[:,\"ImageNum\"] = np.zeros(newDf.shape[0])\n",
    "    indexer = 10**len(str(nImages))\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        name = row[\"ImageFile\"].split(\".\")[0]\n",
    "\n",
    "        seq = int(name.split(\"/\")[0].split(\"q\")[-1])\n",
    "        frame = int(name.split(\"frame\")[-1])\n",
    "\n",
    "        newDf.loc[idx, \"ImageNum\"] = indexer*seq + frame\n",
    "\n",
    "        im = torch.from_numpy(skimage.transform.resize(\n",
    "                                skimage.io.imread(\n",
    "                                    os.path.join(fileDir,\n",
    "                                                    row[\"ImageFile\"])),\n",
    "                                                    size,\n",
    "                                                    mode=\"constant\"))\n",
    "\n",
    "        # extract features and put through GAP layer                                        \n",
    "        im = (im.permute(2,0,1)).reshape(1,3,224,224)\n",
    "        x = model.features(im.float())\n",
    "        newDf.at[idx,\"Features\"] = np.squeeze(torch.nn.AvgPool2d(7)(x).detach().numpy())\n",
    "    \n",
    "    newDf.sort_values(\"ImageNum\", inplace=True)\n",
    "\n",
    "    if outfile:\n",
    "        # use pickle format to make sure values of numpy array\n",
    "        # are saved with full accuracy (saving as txt only yields\n",
    "        # around 9 significant digits, pickle saves bit representation)\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf\n",
    "\n",
    "def define_anchors(df, nAnchors, outfile=None):\n",
    "    \n",
    "    newDf = df.copy()\n",
    "    newDf.reset_index()\n",
    "    \n",
    "    newDf.loc[:,\"AnchorDists\"] = np.zeros((newDf.shape[0], nAnchors, 2)).tolist()\n",
    "    newDf.loc[:,\"AnchorDists\"] = newDf.loc[:,\"AnchorDists\"].astype(object)\n",
    "\n",
    "    myAnchors = np.zeros((nAnchors, 2))\n",
    "\n",
    "    for i, j in enumerate(np.floor(np.linspace(0,df.shape[0], nAnchors, endpoint=False)).astype(int)):\n",
    "        myAnchors[i,:] = np.array([newDf.loc[j, \"X\"], newDf.loc[j,\"Y\"]])\n",
    "    \n",
    "    for index, row in newDf.iterrows():\n",
    "        newDf.at[index, \"AnchorDists\"] = myAnchors - np.array([newDf.loc[index, \"X\"], newDf.loc[index,\"Y\"]])\n",
    "\n",
    "    if outfile:\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf, myAnchors\n",
    "\n",
    "def anchors_for_testSet(df, myAnchors, outfile=None):\n",
    "    # since anchors are only defined on trainings set\n",
    "    newDf = df.copy()\n",
    "\n",
    "    newDf.loc[:,\"AnchorDists\"] = np.zeros((newDf.shape[0], myAnchors.shape[0], 2)).tolist()\n",
    "    newDf.loc[:,\"AnchorDists\"] = newDf.loc[:,\"AnchorDists\"].astype(object)\n",
    "\n",
    "    for index, row in newDf.iterrows():\n",
    "        newDf.at[index, \"AnchorDists\"] = myAnchors - np.array([newDf.loc[index, \"X\"], newDf.loc[index,\"Y\"]])\n",
    "\n",
    "    if outfile:\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pretrained denseNet\n",
    "myModel = torchvision.models.densenet161(pretrained=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\cabal\\Desktop\\Uni\\AML\\prepData.ipynb Zelle 4\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainData \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./ShopFacade/dataset_train.txt\u001b[39m\u001b[39m\"\u001b[39m, delimiter\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, skiprows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainData \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mImageFile\u001b[39m\u001b[39m\"\u001b[39m: trainData\u001b[39m.\u001b[39mloc[:,\u001b[39m\"\u001b[39m\u001b[39mImageFile,\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                   \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m: trainData\u001b[39m.\u001b[39mCamera,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                   \u001b[39m\"\u001b[39m\u001b[39mY\u001b[39m\u001b[39m\"\u001b[39m: trainData\u001b[39m.\u001b[39mPosition,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                   \u001b[39m\"\u001b[39m\u001b[39mQ\u001b[39m\u001b[39m\"\u001b[39m: trainData\u001b[39m.\u001b[39mW,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                   \u001b[39m\"\u001b[39m\u001b[39mR\u001b[39m\u001b[39m\"\u001b[39m: trainData\u001b[39m.\u001b[39mP})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainData \u001b[39m=\u001b[39m extract_feats(myModel, trainData, \u001b[39m\"\u001b[39;49m\u001b[39m./ShopFacade/\u001b[39;49m\u001b[39m\"\u001b[39;49m, outfile\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./ShopFacade/traindata_with_features.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trainData, anch \u001b[39m=\u001b[39m define_anchors(trainData, nAnchors, \u001b[39m\"\u001b[39m\u001b[39m./ShopFacade/traindata_with_features_and_anchors.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m np\u001b[39m.\u001b[39msavetxt(\u001b[39m\"\u001b[39m\u001b[39manchors.txt\u001b[39m\u001b[39m\"\u001b[39m, anch)\n",
      "\u001b[1;32mc:\\Users\\cabal\\Desktop\\Uni\\AML\\prepData.ipynb Zelle 4\u001b[0m in \u001b[0;36mextract_feats\u001b[1;34m(model, df, fileDir, size, outfile, nImages)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m frame \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m newDf\u001b[39m.\u001b[39mloc[idx, \u001b[39m\"\u001b[39m\u001b[39mImageNum\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m indexer\u001b[39m*\u001b[39mseq \u001b[39m+\u001b[39m frame\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m im \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(skimage\u001b[39m.\u001b[39;49mtransform\u001b[39m.\u001b[39;49mresize(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                         skimage\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49mimread(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m                             os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(fileDir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                                             row[\u001b[39m\"\u001b[39;49m\u001b[39mImageFile\u001b[39;49m\u001b[39m\"\u001b[39;49m])),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                                             size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                                             mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mconstant\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# extract features and put through GAP layer                                        \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/cabal/Desktop/Uni/AML/prepData.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m im \u001b[39m=\u001b[39m (im\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cabal\\.conda\\envs\\AML\\lib\\site-packages\\skimage\\transform\\_warps.py:187\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m NumpyVersion(scipy\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m1.6.0\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    185\u001b[0m     \u001b[39m# The grid_mode kwarg was introduced in SciPy 1.6.0\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     zoom_factors \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m factors]\n\u001b[1;32m--> 187\u001b[0m     out \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39;49mzoom(image, zoom_factors, order\u001b[39m=\u001b[39;49morder, mode\u001b[39m=\u001b[39;49mndi_mode,\n\u001b[0;32m    188\u001b[0m                    cval\u001b[39m=\u001b[39;49mcval, grid_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    190\u001b[0m \u001b[39m# TODO: Remove the fallback code below once SciPy >= 1.6.0 is required.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[39m# 2-dimensional interpolation\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(output_shape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m (\u001b[39mlen\u001b[39m(output_shape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    194\u001b[0m                                 output_shape[\u001b[39m2\u001b[39m] \u001b[39m==\u001b[39m input_shape[\u001b[39m2\u001b[39m]):\n",
      "File \u001b[1;32mc:\\Users\\cabal\\.conda\\envs\\AML\\lib\\site-packages\\scipy\\ndimage\\interpolation.py:817\u001b[0m, in \u001b[0;36mzoom\u001b[1;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[0;32m    813\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mdivide(zoom_nominator, zoom_div,\n\u001b[0;32m    814\u001b[0m                     out\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mones_like(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mfloat64),\n\u001b[0;32m    815\u001b[0m                     where\u001b[39m=\u001b[39mzoom_div \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    816\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mascontiguousarray(zoom)\n\u001b[1;32m--> 817\u001b[0m _nd_image\u001b[39m.\u001b[39;49mzoom_shift(filtered, zoom, \u001b[39mNone\u001b[39;49;00m, output, order, mode, cval, npad,\n\u001b[0;32m    818\u001b[0m                      grid_mode)\n\u001b[0;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainData = pd.read_csv(\"./ShopFacade/dataset_train.txt\", delimiter=\" \", skiprows=1)\n",
    "trainData = pd.DataFrame(data={\"ImageFile\": trainData.loc[:,\"ImageFile,\"],\n",
    "                                  \"X\": trainData.Camera,\n",
    "                                  \"Y\": trainData.Position,\n",
    "                                  \"Z\": trainData.loc[:,\"[X\"],\n",
    "                                  \"W\": trainData.Y,\n",
    "                                  \"P\": trainData.Z,\n",
    "                                  \"Q\": trainData.W,\n",
    "                                  \"R\": trainData.P})\n",
    "\n",
    "trainData = extract_feats(myModel, trainData, \"./ShopFacade/\", outfile=\"./ShopFacade/traindata_with_features.pkl\")\n",
    "trainData, anch = define_anchors(trainData, nAnchors, \"./ShopFacade/traindata_with_features_and_anchors.pkl\")\n",
    "np.savetxt(f\"./ShopFacade/anchors{nAnchors}.txt\", anch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"./ShopFacade/dataset_test.txt\", delimiter=\" \", skiprows=1)\n",
    "testData = pd.DataFrame(data={\"ImageFile\": testData.loc[:,\"ImageFile,\"],\n",
    "                                  \"X\": testData.Camera,\n",
    "                                  \"Y\": testData.Position,\n",
    "                                  \"Z\": testData.loc[:,\"[X\"],\n",
    "                                  \"W\": testData.Y,\n",
    "                                  \"P\": testData.Z,\n",
    "                                  \"Q\": testData.W,\n",
    "                                  \"R\": testData.P})\n",
    "\n",
    "testData = extract_feats(myModel, testData, \"./ShopFacade/\", outfile=\"./ShopFacade/testdata_with_features.pkl\")\n",
    "testData = anchors_for_testSet(testData, anch, outfile=\"./ShopFacade/testdata_with_features_and_anchors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('AML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f3fd06b0b48bde2f2f5e7c98d176e850f399d5e50bc95626d8d2defa0349bdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
