{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import os\n",
    "import pickle\n",
    "from scipy.spatial.transform import Rotation\n",
    "import PIL\n",
    "\n",
    "nAnchors = 16\n",
    "batchSize = 24\n",
    "numEpochs = 3\n",
    "device = \"cpu\"\n",
    "rng = np.random.default_rng(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(model, df, fileDir, size=(224,224), outfile=None, nImages=330, numBool = True):\n",
    "    newDf = df.copy()\n",
    "    newDf.loc[:,\"Features\"] = np.zeros((newDf.shape[0], 2208)).tolist()\n",
    "    newDf.loc[:,\"Features\"] = newDf.loc[:,\"Features\"].astype(object)\n",
    "    \n",
    "    # define new index for images where first significant digit is\n",
    "    # sequence number, thus it has to have more overall digits \n",
    "    # than max image number.\n",
    "    if numBool:\n",
    "        newDf.loc[:,\"ImageNum\"] = np.zeros(newDf.shape[0])\n",
    "        indexer = 10**len(str(nImages))\n",
    "    \n",
    "    res = torchvision.transforms.Resize(size)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        name = row[\"ImageFile\"].split(\".\")[0]\n",
    "        \n",
    "        if numBool:\n",
    "            seq = int(name.split(\"/\")[0].split(\"q\")[-1])\n",
    "            frame = int(name.split(\"frame\")[-1])\n",
    "\n",
    "            newDf.loc[idx, \"ImageNum\"] = indexer*seq + frame\n",
    "        im = res(torch.transpose(torch.transpose(torch.from_numpy(skimage.io.imread(os.path.join(fileDir,row[\"ImageFile\"]))), 1,2),0,1))\n",
    "\n",
    "        # extract features and put through GAP layer                                        \n",
    "        im = im.reshape(1,3,224,224)\n",
    "        x = model.features(im.float())\n",
    "        newDf.at[idx,\"Features\"] = np.squeeze(torch.nn.AvgPool2d(7)(x).detach().numpy())\n",
    "    \n",
    "    if numBool:\n",
    "        newDf.sort_values(\"ImageNum\", inplace=True)\n",
    "\n",
    "    if outfile:\n",
    "        # use pickle format to make sure values of numpy array\n",
    "        # are saved with full accuracy (saving as txt only yields\n",
    "        # around 9 significant digits, pickle saves bit representation)\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf\n",
    "\n",
    "def define_anchors(df, nAnchors, outfile=None, randAnchors = False):\n",
    "    \n",
    "    newDf = df.copy()\n",
    "    newDf.reset_index()\n",
    "    \n",
    "    newDf.loc[:,\"AnchorDists\"] = np.zeros((newDf.shape[0], nAnchors, 2)).tolist()\n",
    "    newDf.loc[:,\"AnchorDists\"] = newDf.loc[:,\"AnchorDists\"].astype(object)\n",
    "\n",
    "    myAnchors = np.zeros((nAnchors, 2))\n",
    "\n",
    "    if randAnchors:\n",
    "        anchIdx = rng.permutation(newDf.shape[0])[:nAnchors]\n",
    "        anchDf = (newDf.iloc[anchIdx]).reset_index()\n",
    "        \n",
    "        for index,row in anchDf.iterrows():\n",
    "            myAnchors[index,:] = np.array([row[\"X\"], row[\"Y\"]])\n",
    "\n",
    "    else:\n",
    "        for i, j in enumerate(np.floor(np.linspace(0,df.shape[0], nAnchors, endpoint=False)).astype(int)):\n",
    "            myAnchors[i,:] = np.array([newDf.loc[j, \"X\"], newDf.loc[j,\"Y\"]])\n",
    "    \n",
    "    for index, row in newDf.iterrows():\n",
    "        newDf.at[index, \"AnchorDists\"] = myAnchors - np.array([newDf.loc[index, \"X\"], newDf.loc[index,\"Y\"]])\n",
    "    if outfile:\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf, myAnchors\n",
    "\n",
    "def anchors_for_testSet(df, myAnchors, outfile=None):\n",
    "    # since anchors are only defined on trainings set\n",
    "    newDf = df.copy()\n",
    "\n",
    "    newDf.loc[:,\"AnchorDists\"] = np.zeros((newDf.shape[0], myAnchors.shape[0], 2)).tolist()\n",
    "    newDf.loc[:,\"AnchorDists\"] = newDf.loc[:,\"AnchorDists\"].astype(object)\n",
    "\n",
    "    for index, row in newDf.iterrows():\n",
    "        newDf.at[index, \"AnchorDists\"] = myAnchors - np.array([newDf.loc[index, \"X\"], newDf.loc[index,\"Y\"]])\n",
    "\n",
    "    if outfile:\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf\n",
    "\n",
    "def prep_kaggle_data(location, model, nAnchors, trainTestSplit = 0.9):\n",
    "\n",
    "    kaggleAll = pd.read_csv(location +\"calibration.csv\")\n",
    "    sc = pd.read_csv(\"./kaggle-data/train/scaling_factors.csv\")\n",
    "    sc.index = sc.loc[:,\"scene\"]\n",
    "\n",
    "    translations = np.array([np.array([i.split(\" \")], dtype=float) for i in kaggleAll.loc[:,\"translation_vector\"]])*sc.loc[location.split(\"/\")[-2], \"scaling_factor\"]\n",
    "    \n",
    "    rots = np.array([Rotation.from_matrix(np.array([i.split(\" \")], dtype=float).reshape(3,3)).as_quat() for i in kaggleAll.loc[:,\"rotation_matrix\"]])\n",
    "    \n",
    "    translations = translations.reshape(-1,3)\n",
    "\n",
    "    kaggleAll = pd.DataFrame(data={\"ImageFile\": [kaggleAll.loc[i,\"image_id\"] + \".jpg\" for i in range(kaggleAll.shape[0])],\n",
    "                                \"X\": translations[:,0],\n",
    "                                \"Y\": translations[:,1],\n",
    "                                \"Z\": translations[:,2],\n",
    "                                \"W\": rots[:,0],\n",
    "                                \"P\": rots[:,1],\n",
    "                                \"Q\": rots[:,2],\n",
    "                                \"R\": rots[:,3]})\n",
    "\n",
    "    mySplit = rng.permutation(kaggleAll.shape[0])\n",
    "    lastIndex = int(np.ceil(trainTestSplit * kaggleAll.shape[0]))\n",
    "\n",
    "    kaggleTrain = kaggleAll.iloc[mySplit[:lastIndex]]\n",
    "    kaggleTest = kaggleAll.iloc[mySplit[lastIndex:]]\n",
    "    \n",
    "    kaggleTrain = extract_feats(model, kaggleTrain, location+\"images/\", numBool=False)\n",
    "    kaggleTrain, anch = define_anchors(kaggleTrain, nAnchors, outfile=location+f\"traindata_with_features_and_anchors{nAnchors}.pkl\", randAnchors=True)\n",
    "\n",
    "    kaggleTest = extract_feats(model, kaggleTest, location+\"images/\", numBool=False)             \n",
    "    kaggleTest = anchors_for_testSet(kaggleTest,anch, outfile=location+f\"testdata_with_features_and_anchors{nAnchors}.pkl\")\n",
    "\n",
    "def high_covis_filter(fileDir, quantile=0.1, droppedImages = []):\n",
    "\n",
    "    \"\"\"\n",
    "    fileDir: str to directory containing images\n",
    "    quantile: fraction of images that will be dropped\n",
    "    droppedImages: images that have been dropped in previous steps\n",
    "    \"\"\"\n",
    "    fDrops = droppedImages.copy()\n",
    "    df = pd.read_csv(os.path.join(fileDir, \"pair_covisibility.csv\"))\n",
    "\n",
    "    df.insert(len(df.columns), \"name0\", [i.split(\"-\")[0] for i in df.loc[:,\"pair\"]])\n",
    "    df.insert(len(df.columns), \"name1\", [i.split(\"-\")[1] for i in df.loc[:,\"pair\"]])\n",
    "    df.drop(\"pair\",axis=1, inplace=True)\n",
    "\n",
    "    # delete entries of dropped images\n",
    "    for drop in droppedImages:\n",
    "        df = df[np.logical_and(df.loc[:,\"name0\"] != drop, df.loc[:,\"name1\"] != drop)]\n",
    "    \n",
    "    avgCovis = pd.DataFrame(data={\"image\": df.loc[:,\"name0\"].unique(), \"avgCovisibility\": np.zeros(len(df.loc[:,\"name0\"].unique()))})\n",
    "    \n",
    "    for index, row in avgCovis.iterrows():\n",
    "        totalCovis = df[np.logical_or(df.loc[:,\"name0\"] == row[\"image\"], df.loc[:,\"name1\"] == row[\"image\"])].loc[:,\"covisibility\"]\n",
    "        avgCovis.loc[index,\"avgCovisibility\"] = np.mean(totalCovis)\n",
    "    \n",
    "    fDrops += avgCovis[avgCovis.avgCovisibility < avgCovis.avgCovisibility.quantile(quantile)].image.to_list()\n",
    "    avgCovis = avgCovis[avgCovis.avgCovisibility >= avgCovis.avgCovisibility.quantile(quantile)]\n",
    "\n",
    "    print(\"done\")\n",
    "\n",
    "    return avgCovis, fDrops\n",
    "\n",
    "def randomly_alternate(dfWcovis, fileDir, highest_covis_p = 0.5, occ_size = 0.2, new_pics = 50):\n",
    "\n",
    "    df = dfWcovis.copy()\n",
    "    df = (df[df.avgCovisibility >= df.avgCovisibility.quantile(highest_covis_p)]).reset_index()\n",
    "\n",
    "\n",
    "    randomTrafos = np.array([transforms.RandomErasing(occ_size),\n",
    "                    transforms.RandomInvert(p=0.7), \n",
    "                    transforms.RandomGrayscale(p=0.7),\n",
    "                    transforms.RandomAdjustSharpness(0.5, p = 0.7),\n",
    "                    transforms.RandomAutocontrast(0.5),\n",
    "                    transforms.RandomCrop(0.5),\n",
    "                    ], dtype=object)\n",
    "\n",
    "    nPics = 0\n",
    "\n",
    "    while nPics < new_pics:\n",
    "        idx = rng.integers(0, df.shape[0])\n",
    "        perm = rng.permutation(len(randomTrafos))                \n",
    "\n",
    "        im = torch.from_numpy(skimage.io.imread(os.path.join(fileDir,df.loc[idx, \"image\"] + \".jpg\")).transpose(2,0,1))\n",
    "\n",
    "        T = transforms.Compose([randomTrafos[perm[0]],\n",
    "                            randomTrafos[perm[1]],\n",
    "                            randomTrafos[perm[2]]])\n",
    "    \n",
    "\n",
    "        im = T(im)\n",
    "\n",
    "        newName = str(df.loc[idx, \"image\"]) + \"_aug\" + str(nPics)\n",
    "\n",
    "        im = PIL.Image.fromarray(im.numpy().transpose(1,2,0))\n",
    "\n",
    "        im.save(os.path.join(fileDir, newName+\".jpg\"))\n",
    "        print(\"created new pics\")\n",
    "        nPics += 1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "torch.Size([3, 772, 1036])\n",
      "created new pics\n",
      "torch.Size([3, 1086, 791])\n",
      "created new pics\n",
      "torch.Size([3, 690, 1059])\n",
      "created new pics\n",
      "torch.Size([3, 776, 1046])\n",
      "created new pics\n",
      "torch.Size([3, 608, 822])\n",
      "created new pics\n"
     ]
    }
   ],
   "source": [
    "myDf, dropped = high_covis_filter(\"./kaggle-data/train/sacre_coeur/\", 0.1)\n",
    "randomly_alternate(myDf, \"./kaggle-data/train/sacre_coeur/images\", new_pics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = torchvision.models.densenet161(weights=torchvision.models.DenseNet161_Weights.IMAGENET1K_V1).float()\n",
    "trainSets = {\"./kaggle-data/train/brandenburg_gate/\":30, \"./kaggle-data/train/sacre_coeur/\":25, \"./kaggle-data/train/notre_dame_front_facade/\":82}\n",
    "\n",
    "for location in trainSets.keys():\n",
    "    prep_kaggle_data(location, myModel, trainSets[location], 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pretrained denseNet\n",
    "myModel = torchvision.models.densenet161(weights=torchvision.models.DenseNet161_Weights.IMAGENET1K_V1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"./ShopFacade/dataset_train.txt\", delimiter=\" \", skiprows=1)\n",
    "trainData = pd.DataFrame(data={\"ImageFile\": trainData.loc[:,\"ImageFile,\"],\n",
    "                                  \"X\": trainData.Camera,\n",
    "                                  \"Y\": trainData.Position,\n",
    "                                  \"Z\": trainData.loc[:,\"[X\"],\n",
    "                                  \"W\": trainData.Y,\n",
    "                                  \"P\": trainData.Z,\n",
    "                                  \"Q\": trainData.W,\n",
    "                                  \"R\": trainData.P})\n",
    "\n",
    "trainData = extract_feats(myModel, trainData, \"./ShopFacade/\", outfile=\"./ShopFacade/traindata_with_features.pkl\")\n",
    "trainData, anch = define_anchors(trainData, nAnchors, f\"./ShopFacade/traindata_with_features_and_anchors{nAnchors}.pkl\")\n",
    "np.savetxt(f\"./ShopFacade/anchors{nAnchors}.txt\", anch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"./ShopFacade/dataset_test.txt\", delimiter=\" \", skiprows=1)\n",
    "testData = pd.DataFrame(data={\"ImageFile\": testData.loc[:,\"ImageFile,\"],\n",
    "                                  \"X\": testData.Camera,\n",
    "                                  \"Y\": testData.Position,\n",
    "                                  \"Z\": testData.loc[:,\"[X\"],\n",
    "                                  \"W\": testData.Y,\n",
    "                                  \"P\": testData.Z,\n",
    "                                  \"Q\": testData.W,\n",
    "                                  \"R\": testData.P})\n",
    "\n",
    "testData = extract_feats(myModel, testData, \"./ShopFacade/\", outfile=\"./ShopFacade/testdata_with_features.pkl\")\n",
    "testData = anchors_for_testSet(testData, anch, outfile=f\"./ShopFacade/testdata_with_features_and_anchors{nAnchors}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('AML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "166128ff86268f1fcb3d3420f8f84010dca90d5eb03cf935b19af5b7f3846e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
