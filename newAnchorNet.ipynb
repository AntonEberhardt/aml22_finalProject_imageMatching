{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "nAnchors = 24\n",
    "batchSize = 16\n",
    "numEpochs = 3\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(model, df, fileDir, size=(224,224), outfile=None, nImages=330):\n",
    "    newDf = df.copy()\n",
    "    newDf.loc[:,\"Features\"] = np.zeros((newDf.shape[0], 2208)).tolist()\n",
    "    newDf.loc[:,\"Features\"] = newDf.loc[:,\"Features\"].astype(object)\n",
    "    \n",
    "    # define new index for images where first significant digit is\n",
    "    # sequence number, thus it has to have more overall digits \n",
    "    # than max image number. \n",
    "    newDf.loc[:,\"ImageNum\"] = np.zeros(newDf.shape[0])\n",
    "    indexer = 10**len(str(nImages))\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "        name = row[\"ImageFile\"].split(\".\")[0]\n",
    "\n",
    "        seq = int(name.split(\"/\")[0].split(\"q\")[-1])\n",
    "        frame = int(name.split(\"frame\")[-1])\n",
    "\n",
    "        newDf.loc[idx, \"ImageNum\"] = indexer*seq + frame\n",
    "\n",
    "        im = torch.from_numpy(skimage.transform.resize(\n",
    "                                skimage.io.imread(\n",
    "                                    os.path.join(fileDir,\n",
    "                                                    row[\"ImageFile\"])),\n",
    "                                                    size,\n",
    "                                                    mode=\"constant\"))\n",
    "\n",
    "        # extract features and put through GAP layer                                        \n",
    "        im = (im.permute(2,0,1)).reshape(1,3,224,224)\n",
    "        x = model.features(im.float())\n",
    "        newDf.at[idx,\"Features\"] = np.squeeze(torch.nn.AvgPool2d(7)(x).detach().numpy())\n",
    "    \n",
    "    newDf.sort_values(\"ImageNum\", inplace=True)\n",
    "\n",
    "    if outfile:\n",
    "        # use pickle format to make sure values of numpy array\n",
    "        # are saved with full accuracy (saving as txt only yields\n",
    "        # around 9 significant digits, pickle saves bit representation)\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf\n",
    "\n",
    "def define_anchors(df, nAnchors, outfile=None):\n",
    "    \n",
    "    newDf = df.copy()\n",
    "    newDf.reset_index()\n",
    "    \n",
    "    newDf.loc[:,\"AnchorDists\"] = np.zeros((newDf.shape[0], nAnchors, 2)).tolist()\n",
    "    newDf.loc[:,\"AnchorDists\"] = newDf.loc[:,\"AnchorDists\"].astype(object)\n",
    "\n",
    "    myAnchors = np.zeros((nAnchors, 2))\n",
    "\n",
    "    for i, j in enumerate(np.floor(np.linspace(0,df.shape[0], nAnchors, endpoint=False)).astype(int)):\n",
    "        myAnchors[i,:] = np.array([newDf.loc[j, \"X\"], newDf.loc[j,\"Y\"]])\n",
    "    \n",
    "    for index, row in newDf.iterrows():\n",
    "        newDf.at[index, \"AnchorDists\"] = myAnchors - np.array([newDf.loc[index, \"X\"], newDf.loc[index,\"Y\"]])\n",
    "\n",
    "    if outfile:\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf, myAnchors\n",
    "\n",
    "def anchors_for_testSet(df, myAnchors, outfile=None):\n",
    "    # since anchors are only defined on trainings set\n",
    "    newDf = df.copy()\n",
    "\n",
    "    newDf.loc[:,\"AnchorDists\"] = np.zeros((newDf.shape[0], myAnchors.shape[0], 2)).tolist()\n",
    "    newDf.loc[:,\"AnchorDists\"] = newDf.loc[:,\"AnchorDists\"].astype(object)\n",
    "\n",
    "    for index, row in newDf.iterrows():\n",
    "        newDf.at[index, \"AnchorDists\"] = myAnchors - np.array([newDf.loc[index, \"X\"], newDf.loc[index,\"Y\"]])\n",
    "\n",
    "    if outfile:\n",
    "        newDf.to_pickle(outfile)\n",
    "\n",
    "    return newDf\n",
    "\n",
    "class AnchorDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, file, transform = None, target_transform=None):\n",
    "        super(AnchorDataSet, self).__init__()\n",
    "        self.df = pd.read_pickle(file)\n",
    "        self.transform = transform\n",
    "        self.target_transform=target_transform\n",
    "        \"\"\"\n",
    "        self.filenames = dummy.loc[:,\"ImageFile\"]\n",
    "        self.features = dummy.loc[:,\"Features\"]\n",
    "        self.anchorDists = dummy.loc[:,\"AnchorDists\"]\n",
    "        self.\"\"\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        feats = torch.from_numpy(self.df.loc[idx, \"Features\"]).double()\n",
    "        anchorDists = torch.from_numpy(self.df.loc[idx, \"AnchorDists\"]).double()\n",
    "        \n",
    "        dofs = np.array([self.df.loc[idx,\"Z\"],self.df.loc[idx,\"W\"],self.df.loc[idx,\"P\"],self.df.loc[idx,\"Q\"],self.df.loc[idx,\"R\"]])\n",
    "        dofs = torch.from_numpy(dofs).double()\n",
    "        \n",
    "        xy = torch.from_numpy(np.array([self.df.loc[idx,\"X\"], self.df.loc[idx,\"Y\"]])).double()\n",
    "        imageFile = self.df.loc[idx, \"ImageFile\"]\n",
    "        ImageNum = self.df.loc[idx,\"ImageNum\"]\n",
    "\n",
    "        return {\"Features\": feats,\n",
    "                \"anchorDists\": anchorDists,\n",
    "                \"dofs\": dofs,\n",
    "                \"xy\": xy,\n",
    "                \"ImageFile\": imageFile,\n",
    "                \"ImageNum\": ImageNum}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pretrained denseNet\n",
    "myModel = torchvision.models.densenet161(pretrained=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cabal\\.conda\\envs\\AML\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.read_csv(\"./ShopFacade/dataset_train.txt\", delimiter=\" \", skiprows=1)\n",
    "trainData = pd.DataFrame(data={\"ImageFile\": trainData.loc[:,\"ImageFile,\"],\n",
    "                                  \"X\": trainData.Camera,\n",
    "                                  \"Y\": trainData.Position,\n",
    "                                  \"Z\": trainData.loc[:,\"[X\"],\n",
    "                                  \"W\": trainData.Y,\n",
    "                                  \"P\": trainData.Z,\n",
    "                                  \"Q\": trainData.W,\n",
    "                                  \"R\": trainData.P})\n",
    "\n",
    "trainData = extract_feats(myModel, trainData, \"./ShopFacade/\", outfile=\"./ShopFacade/traindata_with_features.pkl\")\n",
    "trainData, anch = define_anchors(trainData, nAnchors, \"./ShopFacade/traindata_with_features_and_anchors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"./ShopFacade/dataset_test.txt\", delimiter=\" \", skiprows=1)\n",
    "testData = pd.DataFrame(data={\"ImageFile\": testData.loc[:,\"ImageFile,\"],\n",
    "                                  \"X\": testData.Camera,\n",
    "                                  \"Y\": testData.Position,\n",
    "                                  \"Z\": testData.loc[:,\"[X\"],\n",
    "                                  \"W\": testData.Y,\n",
    "                                  \"P\": testData.Z,\n",
    "                                  \"Q\": testData.W,\n",
    "                                  \"R\": testData.P})\n",
    "\n",
    "testData = extract_feats(myModel, testData, \"./ShopFacade/\", outfile=\"./ShopFacade/testdata_with_features.pkl\")\n",
    "testData = anchors_for_testSet(testData, anch, outfile=\"./ShopFacade/testdata_with_features_and_anchors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class anchorNet(nn.Module):\n",
    "    def __init__(self, nAnchors):\n",
    "        super(anchorNet, self).__init__()\n",
    "        self.classifier = nn.Linear(2208, nAnchors)\n",
    "        self.regressor = nn.Linear(2208, 2*nAnchors)\n",
    "        self.dof_regressor = nn.Linear(2208, 5)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.double()\n",
    "    \n",
    "    def forward(self, feats):\n",
    "        classify = self.softmax(self.classifier(nn.functional.relu(feats))) # TODO: ReLU useful?\n",
    "        regress = self.regressor(nn.functional.relu(feats))\n",
    "        dof_regress = self.dof_regressor(nn.functional.relu(feats))\n",
    "\n",
    "        return classify, regress, dof_regress\n",
    "\n",
    "def custom_loss(classify, regress, dof_regress, anchorDistsGt, dofGt, dofLoss,factors = [2.4,0,0.5]):\n",
    "    # TODO: Possible Flaw (or the reason why this works?):\n",
    "    # The net tries to learn realtive position to each anchorpoint\n",
    "    # independently, so we have a large amount of degrees of freedom, even though relative\n",
    "    # position to anchor points should have 2 DOF. Maybe try using more anchorpoints to\n",
    "    # proof this point.\n",
    "\n",
    "    \"\"\"\n",
    "    classify: output of anchor classifier\n",
    "    regress: output of regressor\n",
    "    dof_regress: output of dof_regressor\n",
    "    anchorDistsGt: true distance to all anchor poitns\n",
    "    dofGt: true remaining 4 DOF\n",
    "    dofLoss: loss function used for dof_regressor\n",
    "    factors: list of hyperparameters for weighting of different loss terms\n",
    "    \"\"\"\n",
    "\n",
    "    dist = (regress.reshape(-1, nAnchors, 2) - anchorDistsGt)\n",
    "\n",
    "    # TODO: Use softmax on gt-dof and dof for normalization\n",
    "    lossXY = torch.sum(torch.linalg.norm(dist, axis = -1) * classify)\n",
    "    # TODO: implement cross entropy\n",
    "    \n",
    "    return torch.sum(factors[0] * lossXY) + factors[1] + factors[2] * dofLoss(dof_regress, dofGt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Loss this epoch:  7416.37241208646\n"
     ]
    }
   ],
   "source": [
    "trainDataset = AnchorDataSet(file= \"./ShopFacade/traindata_with_features_and_anchors.pkl\")\n",
    "trainDataloader = torch.utils.data.DataLoader(trainDataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "myNet = anchorNet(nAnchors=nAnchors).to(device)\n",
    "dofLoss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(myNet.parameters(), lr=0.0003)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=80, gamma=0.5)\n",
    "\n",
    "for epoch in range(20):\n",
    "    epochLoss = 0\n",
    "    for i, data in enumerate(trainDataloader):\n",
    "        \n",
    "        myNet.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        classify, regress, dof_regress = myNet.forward(data[\"Features\"])\n",
    "\n",
    "        classify = torch.autograd.Variable(classify, requires_grad=True)\n",
    "        regress = torch.autograd.Variable(regress, requires_grad=True)\n",
    "        dof_regress = torch.autograd.Variable(dof_regress, requires_grad=True)\n",
    "        \n",
    "        loss = custom_loss(classify, regress, dof_regress, anchorDistsGt=data[\"anchorDists\"], dofGt=data[\"dofs\"], dofLoss=dofLoss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epochLoss += loss.data.item()\n",
    "        for param in myNet.parameters():\n",
    "            print(param.grad)\n",
    "    print(\"Loss this epoch: \", epochLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0034e-02, -1.4697e-02,  1.1348e-02,  ...,  1.3746e-03,\n",
      "         -1.2305e-02,  3.2203e-03],\n",
      "        [ 4.5935e-03, -1.5501e-02, -2.0477e-02,  ..., -5.7512e-03,\n",
      "          3.0687e-03,  1.1798e-02],\n",
      "        [-1.3665e-02,  8.0313e-03,  5.4440e-05,  ..., -1.5479e-02,\n",
      "          1.4837e-02,  1.9652e-02],\n",
      "        ...,\n",
      "        [ 1.9303e-03, -5.2583e-03, -3.8259e-03,  ..., -2.9748e-03,\n",
      "         -1.9019e-02,  7.1479e-03],\n",
      "        [-6.0018e-03,  1.7588e-02,  1.5821e-02,  ...,  4.0330e-03,\n",
      "         -2.5783e-03,  1.1901e-02],\n",
      "        [ 1.7472e-02, -1.0161e-02,  7.7957e-03,  ...,  2.0405e-02,\n",
      "         -1.6638e-03,  1.4528e-03]], dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0071,  0.0164, -0.0176,  0.0009, -0.0046,  0.0198, -0.0048,  0.0184,\n",
      "         0.0149,  0.0206, -0.0168, -0.0038, -0.0196, -0.0020,  0.0054, -0.0134,\n",
      "         0.0126,  0.0142, -0.0032, -0.0129,  0.0173, -0.0031,  0.0211,  0.0201],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0212, -0.0060, -0.0205,  ...,  0.0074, -0.0178, -0.0091],\n",
      "        [-0.0022,  0.0045,  0.0057,  ...,  0.0071, -0.0063, -0.0065],\n",
      "        [ 0.0168, -0.0111,  0.0127,  ...,  0.0133,  0.0078, -0.0162],\n",
      "        ...,\n",
      "        [-0.0195, -0.0209, -0.0015,  ..., -0.0173,  0.0003,  0.0035],\n",
      "        [ 0.0138,  0.0057,  0.0014,  ...,  0.0078,  0.0210,  0.0036],\n",
      "        [-0.0042,  0.0084, -0.0167,  ..., -0.0208, -0.0155,  0.0048]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.3101e-03, -4.1042e-03,  1.9968e-03,  3.7586e-03,  2.2844e-03,\n",
      "        -9.2603e-04, -2.8142e-03, -8.7619e-03, -8.5069e-03, -7.0351e-03,\n",
      "         6.1960e-03, -4.3277e-03, -1.2872e-02, -2.0462e-02, -7.3223e-03,\n",
      "        -1.0092e-02, -8.4216e-03, -9.2478e-03,  1.4590e-03, -2.1571e-03,\n",
      "        -2.7071e-03,  2.0057e-02, -5.6134e-03,  8.7057e-03,  1.7504e-03,\n",
      "         1.0276e-02,  8.3662e-03,  1.2605e-02,  1.1479e-02, -5.4775e-05,\n",
      "         9.1771e-03,  8.3683e-03,  7.5962e-03,  2.9590e-03, -9.3942e-03,\n",
      "         8.8736e-03,  7.2593e-03,  1.5144e-02,  2.5116e-03, -3.5587e-03,\n",
      "         8.9117e-03,  1.3204e-02, -8.8116e-03,  1.1381e-02,  1.9848e-02,\n",
      "        -5.8983e-03, -8.1697e-03, -1.6585e-03], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0010, -0.0151, -0.0196,  ...,  0.0099,  0.0132, -0.0098],\n",
      "        [-0.0188,  0.0016, -0.0059,  ..., -0.0210,  0.0108, -0.0039],\n",
      "        [-0.0147,  0.0112, -0.0152,  ...,  0.0102, -0.0012, -0.0016],\n",
      "        [ 0.0042,  0.0116,  0.0200,  ...,  0.0066,  0.0206, -0.0029],\n",
      "        [-0.0047,  0.0037,  0.0034,  ...,  0.0175, -0.0161,  0.0014]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0163,  0.0001,  0.0173, -0.0127, -0.0020], dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in myNet.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('AML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f3fd06b0b48bde2f2f5e7c98d176e850f399d5e50bc95626d8d2defa0349bdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
